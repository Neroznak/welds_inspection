{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6343fe45",
   "metadata": {
    "id": "6343fe45"
   },
   "source": [
    "# Импорт библиотек и обучающих данных:\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871265ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "871265ff",
    "outputId": "8a581d14-5d3c-4840-956e-c7f8607080f5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "import copy\n",
    "from torchvision import transforms, models\n",
    "import os\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2fc707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdef084",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'стоп' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mстоп\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'стоп' is not defined"
     ]
    }
   ],
   "source": [
    "стоп"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75623c10",
   "metadata": {},
   "source": [
    "# Разархивация изображений\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with zipfile.ZipFile(r\"data\\welds.zip\",'r') as zip_obj:\n",
    "#         zip_obj.extractall(r\"data\\binary_root\")\n",
    "data_root = r\"C:\\Users\\HP\\Documents\\Artificial_Intelligence\\Проекты для CV\\weld_inspection\\data\\binary_root\\al5083\\train\"\n",
    "print('Файлы выгрузились из архива успешно!') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ziECZmMoUhw",
   "metadata": {
    "id": "7ziECZmMoUhw"
   },
   "source": [
    "# Разметка данных:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2OuvzFBYoULY",
   "metadata": {
    "id": "2OuvzFBYoULY"
   },
   "source": [
    "У меня есть json файл с разметкой. Надо его пропарсить. Добавляю столбец для класса в случае многоклассовой и бинарной классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rGY-69T2oVd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "rGY-69T2oVd4",
    "outputId": "ce857277-d322-4865-f616-f0ecbc221d85"
   },
   "outputs": [],
   "source": [
    "labels = pd.read_json(r\"C:\\Users\\HP\\Documents\\Artificial_Intelligence\\Проекты для CV\\weld_inspection\\data\\binary_root\\al5083\\train\\train.json\", typ='series')\n",
    "labels = labels.to_frame().reset_index().rename(columns={'index':'path',0:'class'})\n",
    "labels['class'] = labels['class'].replace({0:'good_weld',\n",
    "                                           1:'burn_through',\n",
    "                                           2:'contamination',\n",
    "                                           3:'lack_of_fusion',\n",
    "                                           4:'misalignment',\n",
    "                                           5:'lack_of_penetration'})\n",
    "labels = labels.sort_values(by='class').reset_index().drop('index',axis=1)\n",
    "labels = labels.loc[labels['class'] != 'good_weld']\n",
    "classes = labels['class'].unique()\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c4eef",
   "metadata": {},
   "source": [
    "Для повышения точности модели решено создать две нейросети - первая будет бинарно классифицировать хорошие и плохие швы, а вторая будет определять вид дефекта у плохих швов. Для балансировки данных в бинарной классификации необходимо удвоить количество хороших швов за счёт аугментации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e0808",
   "metadata": {
    "id": "241e0808"
   },
   "source": [
    "# Разбивка данных на обучающие и валидационные:\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab0b02f",
   "metadata": {
    "id": "fab0b02f"
   },
   "source": [
    "Здесь принимаю первое решение: размер валидационной выборки. Он зависит от размера датасета и должен быть случайным. Я хочу использовать порядка 16% датасета для валидации. Валидация случайная - каждый 6-ой файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eae800",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0eae800",
    "outputId": "abb93bdd-2468-4172-caaa-31295f478aee"
   },
   "outputs": [],
   "source": [
    "many_classes_root = r\"data\\many_classes_classifier\"\n",
    "train_dir = os.path.join(many_classes_root,'train')\n",
    "val_dir = os.path.join(many_classes_root,'val')\n",
    "class_names = classes # некий список классов нашей классификации.\n",
    "\n",
    "for dir_name in [train_dir, val_dir]: #итерация через 2 строчки\n",
    "    for class_name in class_names: # для каждого имени класса  в списке классов\n",
    "        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n",
    "print('Успешно созданы папки val и train с классами швов!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s_v8663ICaeB",
   "metadata": {
    "id": "s_v8663ICaeB"
   },
   "source": [
    "Столкнулся с проблемой: при копировании файлов в общую директорию, файлы с одинаковым именем заменяют друг друга.. Надо их как-то переименовывать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ufoXXoF3OfC8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufoXXoF3OfC8",
    "outputId": "9a7db63c-a428-4643-b8b1-7c0a5e025b39"
   },
   "outputs": [],
   "source": [
    "class_names = classes\n",
    "data_root =r\"C:\\Users\\HP\\Documents\\Artificial_Intelligence\\Проекты для CV\\weld_inspection\\data\\binary_root\\al5083\\train\"\n",
    "for class_name in class_names: \n",
    "    for i, file_name in enumerate(tqdm(labels['path'].loc[labels['class']==class_name].tolist())):\n",
    "        pic_name = str(i) + '.png'\n",
    "        if i % 6 != 0:\n",
    "            shutil.copy(os.path.join(data_root, file_name), os.path.join(os.path.join(train_dir, class_name, pic_name)))\n",
    "        else:\n",
    "            shutil.copy(os.path.join(data_root, file_name), os.path.join(os.path.join(val_dir, class_name, pic_name)))\n",
    "print('Изображения отсортированы по train и val!') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k6iMq5x2R48H",
   "metadata": {
    "id": "k6iMq5x2R48H"
   },
   "source": [
    "Проверяем корректность разбивки файлов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iQQKO8uYIeiK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQQKO8uYIeiK",
    "outputId": "e3569233-bcc1-4096-8ad0-cee19f9b53be"
   },
   "outputs": [],
   "source": [
    "lst_train = []\n",
    "lst_val = []\n",
    "for type in classes:\n",
    "    items_train = os.listdir(os.path.join(train_dir,type))\n",
    "    items_val =  os.listdir(os.path.join(val_dir,type))\n",
    "    lst_train.append(len(items_train))\n",
    "    lst_val.append(len(items_val))\n",
    "print('Всего изображений в деле: ', sum(lst_train) + sum(lst_val))\n",
    "print('Всего на обучении: ', sum(lst_train))\n",
    "print('Всего на валидации: ', sum(lst_val))\n",
    "print('Отношение обучающих данных к валидационным: ', sum(lst_train) / sum(lst_val))\n",
    "root = r\"data\\many_classes_classifier\\train\"\n",
    "for cls in classes:\n",
    "    print(cls, len(os.listdir(os.path.join(root,cls))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1485 *2 * 1.7)\n",
    "print(5270)\n",
    "print(3356 * 1.5)\n",
    "print(2349 * 2.2)\n",
    "print(2460 * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a12c1b",
   "metadata": {
    "id": "55a12c1b"
   },
   "source": [
    "# Трансформация и аугментация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "# Путь к директории с изображениями\n",
    "image_dir = r\"data\\many_classes_classifier\\val\\burn_through\"\n",
    "\n",
    "# Получение списка файлов изображений в директории\n",
    "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f)) and f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "file_list = os.listdir(image_dir)\n",
    "random.shuffle(file_list)\n",
    "num_images = len(file_list)\n",
    "num_images_70_percent = int(0.7 * num_images)\n",
    "\n",
    "# Перемешивание списка файлов\n",
    "\n",
    "# Итерация по первым 70% файлов\n",
    "for i in tqdm(range(num_images_70_percent)):\n",
    "    file_name = file_list[i]\n",
    "    image_path = os.path.join(image_dir, file_name)\n",
    "    image = Image.open(image_path)\n",
    "    brightness_augmentation = transforms.ColorJitter(brightness=0.4)\n",
    "    augmented_image = brightness_augmentation(image)\n",
    "    flipped_file_name = \"brightness\" + file_name\n",
    "    flipped_file_path = os.path.join(image_dir, flipped_file_name)\n",
    "    augmented_image.save(flipped_file_path)\n",
    "    \n",
    "# Создание зеркальных копий изображений\n",
    "# for file_name in tqdm(image_files):\n",
    "#     file_path = os.path.join(image_dir, file_name)\n",
    "#     image = Image.open(file_path)\n",
    "#     flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "#     flipped_file_name = \"flipped_\" + file_name\n",
    "#     flipped_file_path = os.path.join(image_dir, flipped_file_name)\n",
    "#     flipped_image.save(flipped_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"data\\many_classes_classifier\\val\\lack_of_fusion\"\n",
    "\n",
    "# Получение списка файлов изображений в директории\n",
    "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f)) and f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "file_list = os.listdir(image_dir)\n",
    "random.shuffle(file_list)\n",
    "num_images = len(file_list)\n",
    "num_images_50_percent = int(0.5 * num_images)\n",
    "\n",
    "# Перемешивание списка файлов\n",
    "\n",
    "# Итерация по первым 50% файлов\n",
    "for i in tqdm(range(num_images_50_percent)):\n",
    "    file_name = file_list[i]\n",
    "    image_path = os.path.join(image_dir, file_name)\n",
    "    image = Image.open(image_path)\n",
    "    brightness_augmentation = transforms.ColorJitter(brightness=0.4)\n",
    "    augmented_image = brightness_augmentation(image)\n",
    "    flipped_file_name = \"brightness\" + file_name\n",
    "    flipped_file_path = os.path.join(image_dir, flipped_file_name)\n",
    "    augmented_image.save(flipped_file_path)\n",
    "    \n",
    "# Создание зеркальных копий изображений\n",
    "# for file_name in tqdm(range(num_images_70_percent)):\n",
    "#     file_path = os.path.join(image_dir, file_name)\n",
    "#     image = Image.open(file_path)\n",
    "#     flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "#     flipped_file_name = \"flipped_\" + file_name\n",
    "#     flipped_file_path = os.path.join(image_dir, flipped_file_name)\n",
    "#     flipped_image.save(flipped_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28425edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"data\\many_classes_classifier\\val\\lack_of_penetration\"\n",
    "\n",
    "# Получение списка файлов изображений в директории\n",
    "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f)) and f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "file_list = os.listdir(image_dir)\n",
    "random.shuffle(file_list)\n",
    "num_images = len(file_list)\n",
    "num_images_20_percent = int(0.2 * num_images)\n",
    "\n",
    "# Перемешивание списка файлов\n",
    "\n",
    "# Итерация по первым 20% файлов\n",
    "for i in tqdm(range(num_images_20_percent)):\n",
    "    file_name = file_list[i]\n",
    "    image_path = os.path.join(image_dir, file_name)\n",
    "    image = Image.open(image_path)\n",
    "    brightness_augmentation = transforms.ColorJitter(brightness=0.4)\n",
    "    augmented_image = brightness_augmentation(image)\n",
    "    flipped_file_name = \"brightness\" + file_name\n",
    "    flipped_file_path = os.path.join(image_dir, flipped_file_name)\n",
    "    augmented_image.save(flipped_file_path)\n",
    "    \n",
    "# Создание зеркальных копий изображений\n",
    "# for file_name in tqdm(image_files):\n",
    "#     file_path = os.path.join(image_dir, file_name)\n",
    "#     image = Image.open(file_path)\n",
    "#     flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "#     flipped_file_name = \"flipped_\" + file_name\n",
    "#     flipped_file_path = os.path.join(image_dir, flipped_file_name)\n",
    "#     flipped_image.save(flipped_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00974ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"data\\many_classes_classifier\\val\\misalignment\"\n",
    "\n",
    "# Получение списка файлов изображений в директории\n",
    "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f)) and f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "file_list = os.listdir(image_dir)\n",
    "# random.shuffle(file_list)\n",
    "# num_images = len(file_list)\n",
    "# num_images_70_percent = int(0.7 * num_images)\n",
    "\n",
    "# Перемешивание списка файлов\n",
    "\n",
    "# Итерация по первым 70% файлов\n",
    "# for i in tqdm(range(num_images_70_percent)):\n",
    "#     file_name = file_list[i]\n",
    "#     image_path = os.path.join(image_dir, file_name)\n",
    "#     image = Image.open(image_path)\n",
    "#     brightness_augmentation = transforms.ColorJitter(brightness=0.4)\n",
    "#     augmented_image = brightness_augmentation(image)\n",
    "#     flipped_file_name = \"brightness\" + file_name\n",
    "#     flipped_file_path = os.path.join(image_dir, flipped_file_name)\n",
    "#     augmented_image.save(flipped_file_path)\n",
    "    \n",
    "# Создание зеркальных копий изображений\n",
    "for file_name in tqdm(image_files):\n",
    "    file_path = os.path.join(image_dir, file_name)\n",
    "    image = Image.open(file_path)\n",
    "    flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    flipped_file_name = \"flipped_\" + file_name\n",
    "    flipped_file_path = os.path.join(image_dir, flipped_file_name)\n",
    "    flipped_image.save(flipped_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ab8cd",
   "metadata": {
    "id": "b18ab8cd"
   },
   "source": [
    "В качестве baseline'а применяется архитектура VGG-19. У данной архитектуры есть требования к исходным изображениям:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30c3afd4",
   "metadata": {
    "id": "30c3afd4"
   },
   "source": [
    "Размер - 224 * 224\n",
    "Пространство RGB\n",
    "Нормализация значений\n",
    "Отсутствие искажений\n",
    "Отсутствие шумов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7a812",
   "metadata": {
    "id": "18f7a812"
   },
   "source": [
    "По этой причине трансформация валидационного изображения будет выглядеть следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233efa06",
   "metadata": {
    "id": "233efa06"
   },
   "outputs": [],
   "source": [
    "many_classes_root = r\"data\\many_classes_classifier\"\n",
    "train_dir = os.path.join(many_classes_root,'train')\n",
    "val_dir = os.path.join(many_classes_root,'val')\n",
    "val_transforms = transforms.Compose([ # трансформации для валидационных изображений:\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "val_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7293d",
   "metadata": {
    "id": "40a7293d"
   },
   "source": [
    "Для обучающих изображений будут применены следующие аугментации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd04847",
   "metadata": {
    "id": "7fd04847"
   },
   "outputs": [],
   "source": [
    "t = 0.0\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    #transforms.RandomHorizontalFlip(), \n",
    "    #transforms.RandomRotation(90),\n",
    "#     transforms.ColorJitter(brightness=t, contrast=t, saturation=t, hue=t),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "])\n",
    "train_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\n",
    "# t = 0.1\n",
    "# train_transforms = transforms.Compose([# дальше идут трансформации всех изображений для обучения, а именно:\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(), #рандомно выполняет \"переворот\" изображения\n",
    "#     transforms.RandomRotation(90),\n",
    "#     transforms.ColorJitter(brightness=t, contrast=t, saturation=t, hue=t),\n",
    "#     transforms.ToTensor(), # преобразует изображение в тензор\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # нормализация для ускорения обучения (среднее = 0, отклонение =1) \n",
    "# ])\n",
    "# train_dataset += torchvision.datasets.ImageFolder(train_dir, train_transforms)\n",
    "# t = 0.2\n",
    "# train_transforms = transforms.Compose([# дальше идут трансформации всех изображений для обучения, а именно:\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(), #рандомно выполняет \"переворот\" изображения\n",
    "#     transforms.RandomRotation(90),\n",
    "#     transforms.ColorJitter(brightness=t, contrast=t, saturation=t, hue=t),\n",
    "#     transforms.ToTensor(), # преобразует изображение в тензор\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # нормализация для ускорения обучения (среднее = 0, отклонение =1) \n",
    "# ])\n",
    "# train_dataset += torchvision.datasets.ImageFolder(train_dir, train_transforms)\n",
    "# t = 0.3\n",
    "# train_transforms = transforms.Compose([# дальше идут трансформации всех изображений для обучения, а именно:\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(), #рандомно выполняет \"переворот\" изображения\n",
    "#     transforms.RandomRotation(90),\n",
    "#     transforms.ColorJitter(brightness=t, contrast=t, saturation=t, hue=t),\n",
    "#     transforms.ToTensor(), # преобразует изображение в тензор\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # нормализация для ускорения обучения (среднее = 0, отклонение =1) \n",
    "# ])\n",
    "# train_dataset += torchvision.datasets.ImageFolder(train_dir, train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a1b2d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10a1b2d1",
    "outputId": "094df347-9b39-4945-a537-b2c314cceac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающих изображений:  24947\n",
      "Валидационных изображений:  5190\n"
     ]
    }
   ],
   "source": [
    "print('Обучающих изображений: ', len(train_dataset))\n",
    "print('Валидационных изображений: ', len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4fd7d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.806743737957611"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset) / len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60948b",
   "metadata": {
    "id": "2c60948b"
   },
   "source": [
    "# Определяю размер batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4877b80",
   "metadata": {
    "id": "b4877b80"
   },
   "source": [
    "При большом числе будет меньше обновлений весов - модель попробует учесть все изображения в одном разе. При малом числе батчей - веса будут обновляться постоянно, но модель будет учитывать малое число изображений. Начнём с 8, буду исследовать для:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aee1e06e",
   "metadata": {
    "id": "aee1e06e"
   },
   "outputs": [],
   "source": [
    "batch_size = 8 # размер бача, т.е. кол-во пикчей, используемых на 1 эпохе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c6d6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af15746",
   "metadata": {
    "id": "1af15746"
   },
   "source": [
    "Применение трансформаций для изображений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdfaceb0",
   "metadata": {
    "id": "cdfaceb0"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff3b59",
   "metadata": {
    "id": "efff3b59"
   },
   "source": [
    "# Определение архитектуры модели:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc4726",
   "metadata": {
    "id": "bfbc4726"
   },
   "source": [
    "Архитектура исследуемой модели представлена ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0bf0874",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0bf0874",
    "outputId": "8e56d570-e25d-41c8-81c0-b774de23d2ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\envs\\gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet169_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet169_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всё ок\n"
     ]
    }
   ],
   "source": [
    "# Загрузка предобученных моделей\n",
    "# model = models.vgg11(pretrained=True)\n",
    "#model = models.vgg19(pretrained=True)\n",
    "#model = models.alexnet(pretrained=True)\n",
    "model = models.densenet169(pretrained=True) \n",
    "#model = models.resnet18(pretrained=True)\n",
    "#model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Замена последнего полносвязного слоя на новый слой для 5 классов\n",
    "num_classes = 5\n",
    "model.classifier._modules['6'] = torch.nn.Linear(4096, num_classes)\n",
    "# Замена последнего полносвязного слоя на новый слой для 6 классов ДЛЯ RESNET\n",
    "# num_classes = 6\n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# Вывод архитектуры модели с новым слоем\n",
    "print('Всё ок')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b68166a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d59e12",
   "metadata": {
    "id": "54d59e12"
   },
   "source": [
    "Ниже определяем лосс-функцию. По дефолту это Кросс-энтропия, т.к в примере была именно она"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f493304b",
   "metadata": {
    "id": "f493304b"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss() # функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee7e5f",
   "metadata": {
    "id": "88ee7e5f"
   },
   "source": [
    "Learning_rate = 0.001 по дефолту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63821a23",
   "metadata": {
    "id": "63821a23"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270d28b",
   "metadata": {
    "id": "5270d28b"
   },
   "source": [
    "Оптимайзер в начале исследования всегда лучше начинать с СГД:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8028b869",
   "metadata": {
    "id": "8028b869"
   },
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# beta1 = 0.9\n",
    "# beta2 = 0.999\n",
    "# eps = 0.00001\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2), eps=eps, amsgrad = True)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.Rmspror(model.parameters(), lr=learning_rate)\n",
    "# alpha = 0.01\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e1157",
   "metadata": {},
   "source": [
    "Хочу отметить, что на этом этапе я усвоил урок - параметры оптимизатора - очень важны. Поменяв alpha для RMprop с 0.9 до 0.01 результат изменился в 3,5 раза."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f317b1c",
   "metadata": {
    "id": "8f317b1c"
   },
   "source": [
    "Уменьшение шага градиента сначала поставим 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70e53f85",
   "metadata": {
    "id": "70e53f85"
   },
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) # уменьшение шага градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4832bbda",
   "metadata": {
    "id": "4832bbda"
   },
   "source": [
    "Число эпох 15, посмотрим как долго будет обучаться модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d50cc4c",
   "metadata": {
    "id": "3d50cc4c"
   },
   "outputs": [],
   "source": [
    "num_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "X3L5QBrR0_ri",
   "metadata": {
    "id": "X3L5QBrR0_ri"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab97c13c",
   "metadata": {
    "id": "ab97c13c"
   },
   "source": [
    "# Обучение модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd455f2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "dd455f2f",
    "outputId": "199dc0a4-36ed-48a8-da47-4fc1d62076ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 1419/3119 [12:40<15:11,  1.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     13\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 14\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Оценка точности на валидационных данных\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\optim\\adagrad.py:123\u001b[0m, in \u001b[0;36mAdagrad.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    119\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    121\u001b[0m     has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, grads, state_sums, state_steps)\n\u001b[1;32m--> 123\u001b[0m     \u001b[43madagrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_sums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\optim\\adagrad.py:223\u001b[0m, in \u001b[0;36madagrad\u001b[1;34m(params, grads, state_sums, state_steps, has_sparse_grad, foreach, differentiable, lr, weight_decay, lr_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adagrad\n\u001b[1;32m--> 223\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_sums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\optim\\adagrad.py:353\u001b[0m, in \u001b[0;36m_multi_tensor_adagrad\u001b[1;34m(params, grads, state_sums, state_steps, lr, weight_decay, lr_decay, eps, has_sparse_grad, maximize, differentiable)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    351\u001b[0m     device_grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_add(device_grads, device_params, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m--> 353\u001b[0m minus_clr \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39mlr \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m lr_decay) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[0;32m    355\u001b[0m device_grads \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_grads]\n\u001b[0;32m    356\u001b[0m device_state_sums \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    357\u001b[0m     torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_state_sums\n\u001b[0;32m    358\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\optim\\adagrad.py:353\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    351\u001b[0m     device_grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_add(device_grads, device_params, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m--> 353\u001b[0m minus_clr \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr_decay\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[0;32m    355\u001b[0m device_grads \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_grads]\n\u001b[0;32m    356\u001b[0m device_state_sums \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    357\u001b[0m     torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_state_sums\n\u001b[0;32m    358\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\_tensor.py:38\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f, assigned\u001b[38;5;241m=\u001b[39massigned)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_torch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "no_improvement_count = 0\n",
    "patience = 5\n",
    "for epoch in range(50):\n",
    "    # Обучение на тренировочных данных\n",
    "    model.train()\n",
    "    for inputs, labels in tqdm(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Оценка точности на валидационных данных\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for inputs, labels in tqdm(val_dataloader):\n",
    "          inputs = inputs.to(device)\n",
    "          labels = labels.to(device)\n",
    "          outputs = model(inputs)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "    \n",
    "    # Проверяем, улучшилась ли точность на этой эпохе\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "        no_improvement_count = 0\n",
    "        print('Модель улучшена!')\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "        print('Модель не улучшилась, обучение продолжается')\n",
    "    \n",
    "    # Если точность не улучшалась в течение patience эпох, прекращаем обучение\n",
    "    if no_improvement_count == patience:\n",
    "        print('No improvement for {} epochs. Stopping training.'.format(patience))\n",
    "        break\n",
    "    \n",
    "    # Выводим информацию о текущей эпохе\n",
    "    print('Epoch {}/{}:'.format(epoch + 1, num_epochs), flush=True)\n",
    "    print('Training Loss: {:.4f}'.format(loss.item()), flush=True)\n",
    "    print('Validation Accuracy: {:.4f}'.format(accuracy), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9ba326",
   "metadata": {
    "id": "ee9ba326"
   },
   "source": [
    "# Сохранение обученной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf33a05",
   "metadata": {
    "id": "bcf33a05"
   },
   "outputs": [],
   "source": [
    "torch.save(best_model, 'desnet169_adagrad_5classes.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c70939",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21595eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = best_model.state_dict()\n",
    "\n",
    "# Вывод весов модели\n",
    "for name, weights in params.items():\n",
    "    print(name, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09892a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
